{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Activity 2 : Extracting specific features from texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a Bag of Word model and a TF-IDF model using sklearn’s ‘The 20 newsgroups text dataset’ using top 20 frequently occuring words. Compare and contrast these two models. Which are the most informative terms of the 2nd document according to these models. Are the sets of most informative terms different from each other? Is so, what do you think are the reasons for this difference? <br>\n",
    "(Note: Assume first instance to be the 0th document )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "import re\n",
    "import string\n",
    "import pandas as pd\n",
    "\n",
    "newsgroups_data_sample = fetch_20newsgroups(subset='train')\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newsgroups_text_df = pd.DataFrame({'text' : newsgroups_data_sample['data']})\n",
    "newsgroups_text_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = stopwords.words('english')\n",
    "\n",
    "#adding individual printable characters to list of stop words so that they get renoved along with the stopwords\n",
    "stop_words = stop_words + list(string.printable)\n",
    "\n",
    "newsgroups_text_df['cleaned_text'] = newsgroups_text_df['text'].apply(\\\n",
    "lambda x : ' '.join([lemmatizer.lemmatize(word.lower()) \\\n",
    "    for word in word_tokenize(re.sub(r'([^\\s\\w]|_)+', ' ', str(x))) if word.lower() not in stop_words]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bag_of_words_model = CountVectorizer(max_features= 20)\n",
    "bag_of_word_df = pd.DataFrame(bag_of_words_model.fit_transform(newsgroups_text_df['cleaned_text']).todense())\n",
    "bag_of_word_df.columns = sorted(bag_of_words_model.vocabulary_)\n",
    "bag_of_word_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_model = TfidfVectorizer(max_features=20)\n",
    "tfidf_df = pd.DataFrame(tfidf_model.fit_transform(newsgroups_text_df['cleaned_text']).todense())\n",
    "tfidf_df.columns = sorted(tfidf_model.vocabulary_)\n",
    "tfidf_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rw = 2\n",
    "list(bag_of_word_df.columns[bag_of_word_df.iloc[rw,:] == bag_of_word_df.iloc[rw,:].max()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rw = 2\n",
    "list(tfidf_df.columns[tfidf_df.iloc[rw,:] == tfidf_df.iloc[rw,:].max()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bag_of_word_df[bag_of_word_df['line']!=0].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bag_of_word_df[bag_of_word_df['edu']!=0].shape[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
