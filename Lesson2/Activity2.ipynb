{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Activity 1 : Extracting General Features from texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract following features from the documents present in the text file ‘data.csv’: <br>\n",
    "i) number of occurrences of each parts of speech, <br>\n",
    "ii) number of punctuations, <br> \n",
    "iii) number of capital and small letter words, <br>\n",
    "iv) number of alphabets, <br>\n",
    "v) number of digits, <br>\n",
    "vi) number of words, <br>\n",
    "vii) number of white spaces for each sentence\n",
    "\n",
    "(Note: Each line is to be treated as a separate document and words starting with uppercase characters are called capital words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from string import punctuation\n",
    "import nltk\n",
    "nltk.download('tagsets')\n",
    "from nltk.data import load\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "from nltk import pos_tag\n",
    "from nltk import word_tokenize\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagdict = load('help/tagsets/upenn_tagset.pickle')\n",
    "list(tagdict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#i) number of occurrences of each parts of speech\n",
    "data = pd.read_csv('data_ch2/data.csv', header = 0)\n",
    "pos_di = {}\n",
    "for pos in list(tagdict.keys()):\n",
    "\tpos_di[pos] = []\n",
    "for doc in data['text']:\n",
    "\tdi = Counter([j for i,j in pos_tag(word_tokenize(doc))])\n",
    "\tfor pos in list(tagdict.keys()):\n",
    "\t\tpos_di[pos].append(di[pos])\n",
    "\n",
    "feature_df = pd.DataFrame(pos_di)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ii) number of punctations\n",
    "feature_df['num_of_unique_punctuations'] = data['text']\\\n",
    ".apply(lambda x : len(set(x).intersection(set(punctuation))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_df['num_of_unique_punctuations'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#iii) number of capital and small letter words\n",
    "feature_df['number_of_capital_words'] =data['text'].apply(lambda x : \\\n",
    "                                            len([word for word in word_tokenize(str(x)) if word[0].isupper()]))\n",
    "feature_df['number_of_capital_words'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_df['number_of_small_words'] =data['text'].apply(lambda x : \\\n",
    "                                            len([word for word in word_tokenize(str(x)) if word[0].islower()]))\n",
    "feature_df['number_of_small_words'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#iv) number of alphabets\n",
    "feature_df['number_of_alphabets'] = data['text'].apply(lambda x : len([ch for ch in str(x) if ch.isalpha()]))\n",
    "feature_df['number_of_alphabets'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#v) number of digits\n",
    "feature_df['number_of_digits'] = data['text'].apply(lambda x : len([ch for ch in str(x) if ch.isdigit()]))\n",
    "feature_df['number_of_digits'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vi) number of words\n",
    "feature_df['number_of_words'] = data['text'].apply(lambda x : len(word_tokenize(str(x))))\n",
    "feature_df['number_of_words'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vii) number of white spaces for each sentence\n",
    "feature_df['number_of_white_spaces'] = data['text'].apply(lambda x : len(str(x).split(' '))-1)\n",
    "feature_df['number_of_white_spaces'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_df.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
