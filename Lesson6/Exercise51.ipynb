{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Rank from Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import re\n",
    "import contractions\n",
    "nltk.download('punkt') # one time execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autosave 120\n",
    "pd.set_option('display.max_colwidth',1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Glove Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "\n",
    "GLOVE_DIR = 'data/glove/'\n",
    "GLOVE_ZIP = GLOVE_DIR + 'glove.6B.50d.zip'\n",
    "\n",
    "zip_ref = zipfile.ZipFile(GLOVE_ZIP, 'r')\n",
    "zip_ref.extractall(GLOVE_DIR)\n",
    "zip_ref.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def load_glove_vectors(fn):\n",
    "    print(\"Loading Glove Model\")\n",
    "    with open( fn,'r', encoding='utf8') as glove_vector_file:\n",
    "        model = {}\n",
    "        for line in glove_vector_file:\n",
    "            parts = line.split()\n",
    "            word = parts[0]\n",
    "            embedding = np.array([float(val) for val in parts[1:]])\n",
    "            model[word] = embedding\n",
    "        print(\"Loaded {} words\".format(len(model)))\n",
    "    return model\n",
    "\n",
    "glove_vectors = load_glove_vectors('data/glove/glove.6B.50d.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles = pd.read_csv(\"data/tennis_articles_v4.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "CLEAN_PATTERN = r'[^a-zA-z\\s]'\n",
    "\n",
    "def clean(word):\n",
    "    return re.sub(CLEAN_PATTERN, '', word)\n",
    "\n",
    "def clean_sentence(sentence):\n",
    "    sentence = [clean(word) for word in sentence]\n",
    "    return [word for word in sentence if word]\n",
    "\n",
    "def clean_sentences(sentences):\n",
    "    return [clean_sentence(sentence) for sentence in sentences]\n",
    "\n",
    "def lower(sentence):\n",
    "    return [word.lower() for word in sentence]\n",
    "\n",
    "def remove_stopwords(sentence):\n",
    "    words = [word for word in sentence if word not in stop_words]\n",
    "    return [word for word in words if len(word) >0]\n",
    "\n",
    "def tokenize_words(sentences):\n",
    "    return [word_tokenize(sentence) \n",
    "              for sentence in sentences]\n",
    "\n",
    "def fix_contractions(sentences):\n",
    "    return [contractions.fix(sentence) for sentence in sentences]\n",
    "\n",
    "articles['SentencesInArticle'] = articles.article_text.apply(sent_tokenize)\n",
    "articles['WordsInSentences'] = articles.SentencesInArticle \\\n",
    "              .apply(fix_contractions)\\\n",
    "              .apply(lower)\\\n",
    "              .apply(tokenize_words)\\\n",
    "              .apply(remove_stopwords)\\\n",
    "              .apply(clean_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles = articles[['SentencesInArticle', 'WordsInSentences']]\n",
    "articles.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VECTOR_SIZE = 50\n",
    "EMPTY_VECTOR = np.zeros(VECTOR_SIZE)\n",
    "\n",
    "def sentence_vector(sentence):\n",
    "    return sum([glove_vectors.get(word, EMPTY_VECTOR) \n",
    "                  for word in sentence])/len(sentence)\n",
    "\n",
    "def sentences_to_vectors(sentences):\n",
    "    return [sentence_vector(sentence) \n",
    "              for sentence in sentences]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Articles With Sentence Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles['SentenceVector'] = \\\n",
    "        articles.WordsInSentences.apply(sentences_to_vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Similarity Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def similarity_matrix(sentence_vectors):\n",
    "    sim_mat = np.zeros([len(sentence_vectors), len(sentence_vectors)])\n",
    "    for i in range(len(sentence_vectors)):\n",
    "        for j in range(len(sentence_vectors)):\n",
    "            element_i = sentence_vectors[i].reshape(1,VECTOR_SIZE)\n",
    "            element_j = sentence_vectors[j].reshape(1,VECTOR_SIZE)\n",
    "            sim_mat[i][j] = cosine_similarity(element_i,\n",
    "                                              element_j)[0,0]\n",
    "    return sim_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles['SimMatrix'] = \\\n",
    "     articles.SentenceVector.apply(similarity_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "\n",
    "def compute_graph(sim_matrix):\n",
    "    nx_graph = nx.from_numpy_array(sim_matrix)\n",
    "    scores = nx.pagerank(nx_graph)\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles['Graph'] = articles.SimMatrix.apply(compute_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ranked Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ranked_sentences(sentences, scores, n=3):\n",
    "    top_scores = sorted(((scores[i],s) \n",
    "                         for i,s in enumerate(sentences)), \n",
    "                                reverse=True)\n",
    "    top_n_sentences = [sentence \n",
    "                        for score,sentence in top_scores[:n]]\n",
    "    return \" \".join(top_n_sentences)\n",
    "\n",
    "articles['Summary'] = articles.apply(lambda d: \n",
    "                                     get_ranked_sentences(d.SentencesInArticle, \n",
    "                                                          d.Graph), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles.loc[0].Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles.loc[1].Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
